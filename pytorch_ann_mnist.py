# -*- coding: utf-8 -*-
"""Pytorch_Ann_Mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jn2qqjT4qBgxRrbFzPYwqe0eG_kc3RkG
"""

import os 
os.chdir('/content/drive/MyDrive/DeepLearningColab/Pytorch_Basics')
!pwd

import matplotlib.pyplot as plt
import numpy as np 
import pandas as pd
import torch 
import torch.nn as nn 
from torchvision import transforms,datasets 
from torch.utils.data import DataLoader
from sklearn.metrics import confusion_matrix,classification_report

plt.style.use('fivethirtyeight')

# Load Data 
train_data = datasets.MNIST(
    root = 'Mnist_data',
    train = True,
    transform = transforms.ToTensor(),
    target_transform = None,
    download = True
)

test_data = datasets.MNIST(
    root = 'Mnist_data',
    train = False,
    transform = transforms.ToTensor(),
    target_transform = None,
    download = True
)

# Check Dataset 
train_data.data[0:2]

train_data.targets

train_data.data.shape

test_data.data.shape

# Exploring Data
plt.figure(figsize=(6,4))
plt.imshow(train_data.data[0:1].reshape(28,28),cmap='gray')
plt.xlabel('Label - '+ str(train_data.targets[0].item()))

# we need to normalize this Before using !
train_data.data.max()

# Normalized Data with torch.round
plt.figure(figsize=(6,4))
plt.imshow(torch.round(train_data.data[1]/255).reshape(28,28),cmap='gray')
plt.xlabel('Label - '+ str(train_data.targets[1].item()))

# Data Loader does : -
# train_data --- reshape -- (samples,color,height,width)
batch = 128
train_data_loader = DataLoader(dataset=train_data,
                               batch_size = batch,
                               shuffle=True)

test_data_loader = DataLoader(dataset=test_data,
                               batch_size = batch)

for x in test_data_loader:
  print(x[0].shape)
  print(x[1].shape)
  break

# Lets Create Model 
device = 'cuda' if torch.cuda.is_available() else 'cpu'

class Neural_network(nn.Module):
  def __init__(self,input,output):
    super(Neural_network,self).__init__()
    self.layers = nn.Sequential(
        nn.Linear(input,256),
        nn.ReLU(),
        nn.Linear(256,128),
        nn.ReLU(),
        nn.Linear(128,output))
    
  def forward(self,x):
    return self.layers(x)

model = Neural_network(784,10)
model.to(device)
model

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

n_epoch = 10
loss_ = []
acc_ = []

for epoch in range(n_epoch):
  print(f'Epoch {epoch+1}/{n_epoch} ')
  for batch,data in enumerate(train_data_loader):
    x = data[0].to(device)
    y = data[1].to(device)
    
    optimizer.zero_grad()

    y_pred = model(x.reshape(-1,784))
    loss = criterion(y_pred,y)
    loss_.append(loss.item())
    loss.backward()
    optimizer.step()
    
    accuracy = torch.mean((torch.argmax(y_pred,1) == y).float()).item()
    acc_.append(accuracy)

    if batch%100 == 0:
      print(f'   Batch {batch} Loss {loss.item():.4f} Accuracy {accuracy:.4f}')

# Only For Train acc and loss 
pd.DataFrame(data={'Loss':loss_,'Accuracy':acc_}).plot()

# Let's test 
test_loss = 0
test_accuracy = 0
with torch.no_grad():
  for batch,data in enumerate(test_data_loader):
    x = data[0].to(device)
    y = data[1].to(device)

    y_pred = model(x.reshape(-1,784))
    loss = criterion(y_pred,y)
    test_loss += loss.item()
    test_accuracy += torch.mean((torch.argmax(y_pred,1) == y).float()).item()
    batch+=1

print(f'Test Loss : {test_loss/batch:.4f} Test_accuracy : {test_accuracy/batch:.4f}')

# Confusion Matrix
pred = np.array([])
target = np.array([])
with torch.no_grad():
  for batch,data in enumerate(test_data_loader):
    x = data[0].to(device)
    y = data[1].to(device)

    y_pred = model(x.reshape(-1,784))
    loss = criterion(y_pred,y)

    pred = np.concatenate((pred,torch.argmax(y_pred,1).cpu().numpy()))
    target = np.concatenate((target,y.cpu().numpy()))

confusion_matrix(target,pred)

