# -*- coding: utf-8 -*-
"""Pytorch_LinearRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YpLsY5uK4UoSeQFpFGgk17sraGJ4ibXC

# Pytorch - Linear Regression 
I have coverd all the approches to train a linear model which includes 
1. Simple linear model 
2. Sequential Implementation (Added Non linearity )
   (Wanted you to see effect of NL:)
3. Functional Implementation (Added Non linearity )

Hope You will find it interesting and point to point.!
"""

# Basic Imports  
import torch           # Main torch lib 
import torch.nn as nn  # nn module contains all usefull functions 
import numpy as np     
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
from torch.utils.data import TensorDataset , DataLoader 

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Uploading csv file on colab for Linear Regression 
# cement Slump Data 
from google.colab import files
files.upload()

"""# UCI REPOSITORY 
# To show Linear regression I am using cement slump dataset 
## Official link : https://archive.ics.uci.edu/ml/datasets/concrete+slump+test

# Data Preparation
"""

!ls

df = pd.read_csv('cement_slump.csv')
df.head()

# No missing Values -- so we can directly start working on model building and training 
df.isnull().sum()

# Data - Target Split 
# Here we have 3 Target Variables 
X = df.drop(['SLUMP(cm)','FLOW(cm)','Compressive Strength (28-day)(Mpa)'],axis=1)
y = df[['SLUMP(cm)','FLOW(cm)','Compressive Strength (28-day)(Mpa)']]

X.head()

y.head()

# By default pytorch uses float32 Tensor and numpy uses float64 
# Float 32 is much faster in operations than float64 
# So we will convert ndarry to pytorch tensor 

# Current dtype
print(X.values.dtype)
print(y.values.dtype)

# Pytorch uses shape(rows,cols) so reshaped X and y 
X = X.values.astype(np.float32).reshape(X.shape[0],-1)
y = y.values.astype(np.float32).reshape(y.shape[0],3)

# Converted nd.array into torch tensor 
X = torch.from_numpy(X)
y = torch.from_numpy(y)

print(X.dtype)
print(type(X))

# Lets do the Train - Test split 
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=101)
len(X_train) , len(y_train) , len(X_test) , len(y_test)

# Lets Create Dataloader so we can get a batch of 32 
# To do this first we need to create a iterable of x and y 
# Which we can do with TensorDataset 

train_data = TensorDataset(X_train,y_train)
test_data = TensorDataset(X_test,y_test)

train_data[0:1]

# Since it is now tuple of tensor we can load it in dataloader to get batches
train_data_loader = DataLoader(train_data,batch_size=32)
# we want to test whole data at once so create batch size = size of data set here 26
test_data_loader = DataLoader(test_data,batch_size=26)

"""# Model Creation and Training """

# I have created linear model 
# That why i wont be using activation function 
# since we know that without activation function whole neural network is a linear model 
# y = w1x1 + w2x2 + ..... w7X7

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

class Linear_NN(nn.Module):
  def __init__(self,input_shape,output_shape):
    super(Linear_NN,self).__init__()
    self.Layer = nn.Linear(input_shape,output_shape)

  def forward(self,input):
    return self.Layer(input)

model = Linear_NN(X.shape[1],y.shape[1])
model

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters())

# Lets Train Model  
n_epoch = 1000
loss_matrix =[]
for epoch in range(n_epoch):
  if epoch%100 ==0:
    print(f'Epoch {epoch}/{n_epoch} ')

  for batch , (input,output) in enumerate(train_data_loader):

    input.device_ = device 
    output.device_ = device
   
    optimizer.zero_grad()            # Removing previously stored Gradient 
    y_pred = model.forward(input)    # Feed Forward Network 
    loss = criterion(output,y_pred)  # Loss Calculation 
    loss.backward()                  # d(loss)/dw 
    optimizer.step()                 # w = w - nd(loss)/dw
    if epoch%100 == 0:
     print(f'    Batch : {batch}  Loss : {loss.item()} ')

  loss_matrix.append(loss.item())

# Lets test Our model Now 
with torch.no_grad():
  for batch , (input,output) in enumerate(test_data_loader):
    input.device_ = device 
    output.device_ = device

    y_pred = model.forward(input)
    loss = criterion(output,y_pred)
    
    print(f'Loss : {loss.item()}')

# Test our model on Custom Data 
# since we have 7 Columns lets generate 7 random values 
data = torch.randint(10,50,(7,)).reshape(1,7)

# Getting 3 values at output 
# so our model is working fine 
model.forward(data.float())

"""# Lets Implement Non Linear Model (Sequential Implementation)"""

# Sequential implementation of seq model 
class Non_Linear_NN(nn.Module):
  def __init__(self,input_shape,output_shape):
    super(Non_Linear_NN,self).__init__()
    self.Layers = nn.Sequential(
        nn.Linear(input_shape,24),
        nn.ReLU(),
        nn.Linear(24,output_shape),
    )
  
  def forward(self,input):
    return self.Layers(input)

model = Non_Linear_NN(X.shape[1],y.shape[1])
model

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(),lr=0.001)

# Lets Train Model  
n_epoch = 200
loss_matrix =[]
for epoch in range(n_epoch):
  if epoch%50 == 0:
    print(f'Epoch {epoch}/{n_epoch} ')
  
  for batch , (input,output) in enumerate(train_data_loader):
    input.device_ = device 
    output.device_ = device

    optimizer.zero_grad()            # Removing previously stored Gradient 
    y_pred = model.forward(input)    # Feed Forward Network 
    loss = criterion(output,y_pred)  # Loss Calculation 
    loss.backward()                  # d(loss)/dw 
    optimizer.step()                 # w = w - nd(loss)/dw

    if epoch%50==0:
      print(f'    Batch : {batch}  Loss : {loss.item()} ')

  loss_matrix.append(loss.item())

# Lets test Our model Now 
with torch.no_grad():
  for batch , (input,output) in enumerate(test_data_loader):
    input.device_ = device 
    output.device_ = device

    y_pred = model.forward(input)
    loss = criterion(output,y_pred)
    
    print(f'Loss : {loss.item()}')

# Test our model on Custom Data 
# since we have 7 Columns lets generate 7 random values 
data = torch.randint(10,50,(7,)).reshape(1,7)

model.forward(data.float())

"""# Lets Implement Non Linear Model (Functional Implementation)"""

# Functional Implementation 
class Non_Linear_NN(nn.Module):
  def __init__(self,input_shape,output_shape):
    super(Non_Linear_NN,self).__init__()
    self.Layer1 = nn.Linear(input_shape,14)
    self.activation = nn.ReLU()
    self.Layer2 = nn.Linear(14,output_shape)
  
  def forward(self,input):
    x = self.Layer1(input)
    x = self.activation(x)
    x = self.Layer2(x)
    return x

model = Non_Linear_NN(X.shape[1],y.shape[1])
model

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(),lr=0.001)

# Lets Train Model  
n_epoch = 200
loss_matrix =[]
for epoch in range(n_epoch):
  if epoch%50 == 0:
    print(f'Epoch {epoch}/{n_epoch} ')
  
  for batch , (input,output) in enumerate(train_data_loader):
    input.device_ = device 
    output.device_ = device

    optimizer.zero_grad()            # Removing previously stored Gradient 
    y_pred = model.forward(input)    # Feed Forward Network 
    loss = criterion(output,y_pred)  # Loss Calculation 
    loss.backward()                  # d(loss)/dw 
    optimizer.step()                 # w = w - nd(loss)/dw

    if epoch%50==0:
      print(f'    Batch : {batch}  Loss : {loss.item()} ')

  loss_matrix.append(loss.item())

# Lets test Our model Now 
with torch.no_grad():
  for batch , (input,output) in enumerate(test_data_loader):
    input.device_ = device 
    output.device_ = device

    y_pred = model.forward(input)
    loss = criterion(output,y_pred)
    
    print(f'Loss : {loss.item()}')

# Test our model on Custom Data 
# since we have 7 Columns lets generate 7 random values 
data = torch.randint(10,50,(7,)).reshape(1,7)
model.forward(data.float())

"""# Conclusion 

1. When i trained linear model it took 1000 epoch to get a good accuracy.
2. But when i introduced non Linearity it got good accuracy in just 400 epochs.

"""