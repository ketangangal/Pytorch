{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_IMPLEMENTATION.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yFO1mCuimLNZ"
      },
      "outputs": [],
      "source": [
        "# Baisc Implementation \n",
        "import torch \n",
        "import torch.nn as nn  \n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self,img_dim):\n",
        "    super(Discriminator,self).__init__()\n",
        "    self.disc = nn.Sequential(\n",
        "        nn.Linear(img_dim,128),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(128,1),\n",
        "        nn.Sigmoid())\n",
        "    \n",
        "  def forward(self,x):\n",
        "    return self.disc(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim, img_dim):\n",
        "    super(Generator,self).__init__()\n",
        "    self.gen = nn.Sequential(\n",
        "        nn.Linear(z_dim,256),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(256,img_dim),\n",
        "        nn.Tanh())\n",
        "    \n",
        "  def forward(self,x):\n",
        "    return self.gen(x)\n",
        "\n",
        "\n",
        "# Gan Are sensitive to hyperparameters \n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lr = 3e-4   \n",
        "z_dim = 64\n",
        "img_dim = 28*28*1\n",
        "batch_size = 32\n",
        "num_epoch = 50\n",
        "\n",
        "disc = Discriminator(img_dim).to(device)\n",
        "gen = Generator(z_dim,img_dim).to(device)\n",
        "\n",
        "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])\n",
        "dataset = datasets.MNIST(root='dataset/', transform=transform, download=True)\n",
        "loader = DataLoader(dataset,batch_size=batch_size, shuffle=True)\n",
        "\n",
        "optimizer_disc = optim.Adam(disc.parameters(),lr=lr)\n",
        "optimizer_gen = optim.Adam(gen.parameters(),lr=lr)\n",
        "\n",
        "critrion = nn.BCELoss()\n",
        "\n",
        "writer_fake = SummaryWriter(f'GAN_MNIST/fake')\n",
        "writer_real = SummaryWriter(f'GAN_MNIST/real')\n",
        "\n",
        "step = 0\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "  for batch, (real, _) in enumerate(loader):\n",
        "    real = real.view(-1,784).to(device)\n",
        "    batch_size =  real.shape[0]\n",
        "\n",
        "    # Train Discriminator -- > Loss --> Max E(D)log(D(real)) + E(G)log(1-D(G(fake)))\n",
        "    optimizer_disc.zero_grad()\n",
        "\n",
        "    noise = torch.randn((batch_size,z_dim)).to(device)\n",
        "    fake = gen.forward(noise)\n",
        "\n",
        "    disc_real = disc(real).view(-1)\n",
        "    lossD_real = critrion(disc_real, torch.ones_like(disc_real))\n",
        "\n",
        "    disc_fake = disc(fake).view(-1)\n",
        "    lossD_fake = critrion(disc_fake, torch.zeros_like(disc_fake))\n",
        "\n",
        "    lossD = (lossD_real + lossD_fake)/2\n",
        "\n",
        "    lossD.backward(retain_graph = True)\n",
        "    optimizer_disc.step()\n",
        "\n",
        "    # Train Generator --> Min E(G)log(1-D(G(fake))) --- > Max (E(G)log(D(G(fake))))\n",
        "    optimizer_gen.zero_grad()\n",
        "\n",
        "    output = disc(fake).view(-1)\n",
        "    lossG = critrion(output, torch.ones_like(output))\n",
        "    lossG.backward()\n",
        "\n",
        "    optimizer_gen.step()\n",
        "\n",
        "    # Print Loss\n",
        "    if batch == 0:\n",
        "      print(f'Epoch {epoch}/{num_epoch}  Discriminator_Loss {lossD:.4f}  Generator_Loss {lossG:.4f} ')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      fake = gen(fixed_noise).reshape(-1,1,28,28)\n",
        "      data = real.reshape(-1,1,28,28)\n",
        "\n",
        "      img_grid_fake = torchvision.utils.make_grid(fake,normalize=True)\n",
        "      img_grid_real = torchvision.utils.make_grid(real,normalize=True)\n",
        "\n",
        "      writer_fake.add_image(\"Mnist Fake Image \", img_grid_fake, global_step=step)\n",
        "      writer_real.add_image(\"Mnist Real Image \", img_grid_real, global_step=step)\n",
        "\n",
        "      step +=1\n"
      ],
      "metadata": {
        "id": "d4AAZ5FHnmEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KNUlBBvK5qrK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}