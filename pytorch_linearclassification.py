# -*- coding: utf-8 -*-
"""Pytorch_LinearClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xeZVeZVbywlYImRpQrmp0AdEsxfOCNKh
"""

# All Necessary Imports
import torch 
import torch.nn as nn
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import TensorDataset, DataLoader

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import plot_confusion_matrix,classification_report, accuracy_score , confusion_matrix

# Importing Dataset into colab 
from google.colab import files
data = files.upload()

!ls

# Importing Dataset into Dataframe 
df = pd.read_csv('iris.csv')
df.head()

df.isnull().sum()

# Here You can see that we have 3 classes 
# Here -> we can either use label encoding in target or one hot encoding.
# Label Encoding will create one target column 
# One hot encoding will create three target columns 

# I will show both approches along with sequential and functional implementation. 
df.species.unique()

"""# Data Preparation """

# Now we all know that torch uses float32 tensor and numpy has float64 
# Also dimension should be (Rows, cols) so I wll reshape Data 
# With small dataset you will ANN generally overfits but here we will ignore this As we are learning 
len(df)

X = df.drop('species',axis=1)
y = df['species']

X.head()

y.head()

# Label Encoding : -
# Pytorch supports label encoding but it should start from 0 to n 
# Here we have 3 species so we will create target map 

target_map = {'setosa':0,'versicolor':1,'virginica':2}
target_map

y.apply(lambda x: target_map[x])

y = y.apply(lambda x: target_map[x])

# Now let's Reshape and convert data
# Dont run this cell twice , It will give Error 
X = X.values.reshape(X.shape[0],X.shape[1]).astype(np.float32)
y = y.values.reshape(y.shape[0],-1).astype(np.float32)

print('Shape :', X.shape,' Dtype:', X.dtype)
print('Shape :', y.shape,' Dtype:', y.dtype)

X[0:5,:]

y[0:5]

# Convert numpy to tensor 
print(type(X))
print(type(y))

print('--Conversion---')
X = torch.from_numpy(X)
y = torch.from_numpy(y)
print(type(X))
print(type(y))

# Now lets split Data into Train and Test 
X_train, X_test , y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)
len(X_train), len(X_test) , len(y_train), len(y_test)

# iterable Creation 
train_data = TensorDataset(X_train,y_train)
test_data = TensorDataset(X_test,y_test)

# loader Creation 
train_data_loader = DataLoader(train_data,batch_size=32,shuffle=True)
test_data_loader = DataLoader(test_data,batch_size=38)

"""# Let's Create and Train Model 
### Type 1 : Linear classification simple linear model
"""

# Linear Model -- > Contains Only 1 Input and 1 output layer 
 # This model can only generate st lines 
 class Linear_NN(nn.Module):
   def __init__(self,input_shape,output_shape):
     super(Linear_NN,self).__init__()
     self.layers = nn.Linear(input_shape,output_shape)
   def forward(self,x):
     return self.layers(x)

model = Linear_NN(4,3)
model

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

n_epoch = 500
train_acc = []
train_loss = []

for epoch in range(n_epoch):
  if epoch%100 == 0:
    print(f'Epoch : {epoch+1}/{n_epoch} ')

  for batch , data in enumerate(train_data_loader):
    x = data[0]
    y = data[1]
    
    optimizer.zero_grad()   # Removing accumulation of gradients 
    y_pred = model(x)       # model forward pass
    loss = criterion(y_pred,y.squeeze().long())  # loss calculation 
    train_loss.append(loss.item())  
    loss.backward()  # df/dx
    optimizer.step() # w = w - n*df/dx

    
    with torch.no_grad():
      accuracy = (torch.argmax(y_pred,axis=1) == y.squeeze().long()).float().mean()
    train_acc.append(accuracy)

    if epoch%100 == 0:
      print(f'    Batch : {batch}  Loss : {loss.item():.4f}  Accuracy :{accuracy}')

with torch.no_grad():
  x,y = test_data[0:]
  y_pred = model(x)
  loss = criterion(y_pred,y.squeeze().long())

  accuracy = (torch.argmax(y_pred,axis=1) == y.squeeze().long()).float().mean()

  print(f'Test Loss : {loss.item():.4f}  Accuracy : {accuracy} ')

# Test 
test_ = torch.tensor([5.1,3.5,1.4,0.2]).reshape(1,4)
test_

result = model(test_)
print('Actual Target : ',target_map.items(),'\nResult : ',torch.argmax(result,axis=1).item())

print(classification_report(torch.argmax(y_pred,axis=1),y.squeeze().long()))

"""### Type 2 : Linear classification Non Linear Model (Sequential)"""

class Linear_NN(nn.Module):
   def __init__(self,input_shape,output_shape):
     super(Linear_NN,self).__init__()
     self.layers = nn.Sequential(nn.Linear(input_shape,16),
                                 nn.Sigmoid(),
                                 nn.Linear(16,output_shape))
   def forward(self,x):
     return self.layers(x)

model = Linear_NN(4,3)
model

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

n_epoch = 500
train_acc = []
train_loss = []

for epoch in range(n_epoch):
  if epoch%100 == 0:
    print(f'Epoch : {epoch+1}/{n_epoch} ')

  for batch , data in enumerate(train_data_loader):
    x = data[0]
    y = data[1]
    
    optimizer.zero_grad()   # Removing accumulation of gradients 
    y_pred = model(x)       # model forward pass
    loss = criterion(y_pred,y.squeeze().long())  # loss calculation 
    train_loss.append(loss.item())  
    loss.backward()  # df/dx
    optimizer.step() # w = w - n*df/dx

    
    with torch.no_grad():
      accuracy = (torch.argmax(y_pred,axis=1) == y.squeeze().long()).float().mean()
    train_acc.append(accuracy)

    if epoch%100 == 0:
      print(f'    Batch : {batch}  Loss : {loss.item():.4f}  Accuracy :{accuracy}')

with torch.no_grad():
  x,y = test_data[0:]
  y_pred = model(x)
  loss = criterion(y_pred,y.squeeze().long())

  accuracy = (torch.argmax(y_pred,axis=1) == y.squeeze().long()).float().mean()

  print(f'Test Loss : {loss.item():.4f}  Accuracy : {accuracy} ')

print(classification_report(torch.argmax(y_pred,axis=1),y.squeeze().long()))

print(confusion_matrix(torch.argmax(y_pred,axis=1),y.squeeze().long()))

"""### Type 3 : Linear classification Non Linear Model (Functional)"""

class Linear_NN(nn.Module):
   def __init__(self,input_shape,output_shape):
     super(Linear_NN,self).__init__()
     self.layer1 = nn.Linear(input_shape,16)
     self.activation = nn.Sigmoid()
     self.layer2 = nn.Linear(16,output_shape)

   def forward(self,x):
     x = self.layer1(x)
     x = self.activation(x)
     x = self.layer2(x)
     return x

model = Linear_NN(4,3)
model

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

n_epoch = 500
train_acc = []
train_loss = []

for epoch in range(n_epoch):
  if epoch%100 == 0:
    print(f'Epoch : {epoch+1}/{n_epoch} ')

  for batch , data in enumerate(train_data_loader):
    x = data[0]
    y = data[1]
    
    optimizer.zero_grad()   # Removing accumulation of gradients 
    y_pred = model(x)       # model forward pass
    loss = criterion(y_pred,y.squeeze().long())  # loss calculation 
    train_loss.append(loss.item())  
    loss.backward()  # df/dx
    optimizer.step() # w = w - n*df/dx

    
    with torch.no_grad():
      accuracy = (torch.argmax(y_pred,axis=1) == y.squeeze().long()).float().mean()
    train_acc.append(accuracy)

    if epoch%100 == 0:
      print(f'    Batch : {batch}  Loss : {loss.item():.4f}  Accuracy :{accuracy}')

with torch.no_grad():
  x,y = test_data[0:]
  y_pred = model(x)
  loss = criterion(y_pred,y.squeeze().long())

  accuracy = (torch.argmax(y_pred,axis=1) == y.squeeze().long()).float().mean()

  print(f'Test Loss : {loss.item():.4f}  Accuracy : {accuracy} ')

print(classification_report(torch.argmax(y_pred,axis=1),y.squeeze().long()))

print(confusion_matrix(torch.argmax(y_pred,axis=1),y.squeeze().long()))

# Conclusion 
# 1 . Linear model took 500 epochs to get good accuracy but could reach up to 80-90%
# 2. Non linear took same epoch but able to get accuracy of 100% 

# Data has some non linearity in it which we are able to find with the help of Non linear model

